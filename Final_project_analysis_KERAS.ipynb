{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "es06OrDiMYaa",
        "6XWPHMCqM31A",
        "2eafsvCEOGHb",
        "YZ1VkAXcON_7",
        "NmGVdULaOgEa",
        "8CvEX1J9Oq8A",
        "VhNr8DY1Ox2Z"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## checking gpus\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4i9sbmnX0Rt",
        "outputId": "ee06d051-456c-4662-a26d-3462c7fe5fdb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov 30 21:35:59 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8-GsJN6n3czK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "### Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "#import torchviz\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "# %matplotlib notebook\n",
        "%matplotlib inline\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler    \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "pd.set_option(\"display.max_columns\", 50, \"display.max_rows\", 100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required modules\n",
        "# final_project/jsons_data.csv.zip\n",
        "# /content/jsons_data.csv.zip\n",
        "# /jsons_data.csv.zip\n",
        "#from zipfile import ZipFile\n",
        "from zipfile import ZipFile \n",
        "#/content/jsons_data.csv.zip\n",
        "  \n",
        "# specifying the zip file name\n",
        "file_name = \"/content/jsons_data.csv.zip\"\n",
        "  \n",
        "# opening the zip file in READ mode\n",
        "with ZipFile(file_name, 'r') as new:\n",
        "    # printing all the contents of the zip file\n",
        "    new.printdir()\n",
        "  \n",
        "    # extracting all the files\n",
        "    print('Extracting all the files now...')\n",
        "    new.extractall()\n",
        "    print('Done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC5FKg-oHrdI",
        "outputId": "fcfc8664-1095-4e56-a889-ad83843544d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Name                                             Modified             Size\n",
            "jsons_data.csv                                 2022-11-24 13:40:00     65882315\n",
            "__MACOSX/._jsons_data.csv                      2022-11-24 13:40:00          233\n",
            "Extracting all the files now...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "KDSuN6NU8y7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# final_project/jsons_data.csv\n",
        "df = pd.read_csv(\"/content/jsons_data.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "Ra_i9zYe83hD",
        "outputId": "d1fe93fa-70ef-4547-8097-b4e173c265ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                           abstract  \\\n",
              "0           0  Glioblastoma is the most common and most aggre...   \n",
              "1           1  Chiral amino alcohols are structural motifs pr...   \n",
              "2           2  A new dengue vaccine was associated with incre...   \n",
              "3           3  This paper explores the tension between post-f...   \n",
              "4           4  A key strategy to enable training of deep neur...   \n",
              "\n",
              "                                               title  \\\n",
              "0  PEITC induces apoptosis of Human Brain Gliobla...   \n",
              "1  Multi-step biocatalytic strategies for chiral ...   \n",
              "2  Dengue vaccine safety signal: Immune enhanceme...   \n",
              "3  Achieving respectable motherhood? Exploring th...   \n",
              "4  Soft++, a multi-parametric non-saturating non-...   \n",
              "\n",
              "                                            keywords  \n",
              "0  ['Apoptosis', 'Caspases', 'Glioblastoma', 'Mit...  \n",
              "1  ['Cascades', 'Chiral amino alcohols', 'Recycli...  \n",
              "2  ['Adverse events following immunization', 'Cli...  \n",
              "3                                                NaN  \n",
              "4  ['Activation function', 'Convolutional neural ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-679bab1d-3ffe-4a8c-9fe0-abc8dd019ee9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>abstract</th>\n",
              "      <th>title</th>\n",
              "      <th>keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Glioblastoma is the most common and most aggre...</td>\n",
              "      <td>PEITC induces apoptosis of Human Brain Gliobla...</td>\n",
              "      <td>['Apoptosis', 'Caspases', 'Glioblastoma', 'Mit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Chiral amino alcohols are structural motifs pr...</td>\n",
              "      <td>Multi-step biocatalytic strategies for chiral ...</td>\n",
              "      <td>['Cascades', 'Chiral amino alcohols', 'Recycli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>A new dengue vaccine was associated with incre...</td>\n",
              "      <td>Dengue vaccine safety signal: Immune enhanceme...</td>\n",
              "      <td>['Adverse events following immunization', 'Cli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>This paper explores the tension between post-f...</td>\n",
              "      <td>Achieving respectable motherhood? Exploring th...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>A key strategy to enable training of deep neur...</td>\n",
              "      <td>Soft++, a multi-parametric non-saturating non-...</td>\n",
              "      <td>['Activation function', 'Convolutional neural ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-679bab1d-3ffe-4a8c-9fe0-abc8dd019ee9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-679bab1d-3ffe-4a8c-9fe0-abc8dd019ee9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-679bab1d-3ffe-4a8c-9fe0-abc8dd019ee9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[[\"abstract\", \"title\"]]\n",
        "df = df.applymap(str)\n",
        "# only preserve needed columns\n",
        "df.head()"
      ],
      "metadata": {
        "id": "PCsUvue686dv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "968e415b-e9e1-4487-b835-4722af4a60c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            abstract  \\\n",
              "0  Glioblastoma is the most common and most aggre...   \n",
              "1  Chiral amino alcohols are structural motifs pr...   \n",
              "2  A new dengue vaccine was associated with incre...   \n",
              "3  This paper explores the tension between post-f...   \n",
              "4  A key strategy to enable training of deep neur...   \n",
              "\n",
              "                                               title  \n",
              "0  PEITC induces apoptosis of Human Brain Gliobla...  \n",
              "1  Multi-step biocatalytic strategies for chiral ...  \n",
              "2  Dengue vaccine safety signal: Immune enhanceme...  \n",
              "3  Achieving respectable motherhood? Exploring th...  \n",
              "4  Soft++, a multi-parametric non-saturating non-...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4f258c5a-4849-4d95-b682-1b2b468c0e7b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abstract</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Glioblastoma is the most common and most aggre...</td>\n",
              "      <td>PEITC induces apoptosis of Human Brain Gliobla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Chiral amino alcohols are structural motifs pr...</td>\n",
              "      <td>Multi-step biocatalytic strategies for chiral ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A new dengue vaccine was associated with incre...</td>\n",
              "      <td>Dengue vaccine safety signal: Immune enhanceme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This paper explores the tension between post-f...</td>\n",
              "      <td>Achieving respectable motherhood? Exploring th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A key strategy to enable training of deep neur...</td>\n",
              "      <td>Soft++, a multi-parametric non-saturating non-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f258c5a-4849-4d95-b682-1b2b468c0e7b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f258c5a-4849-4d95-b682-1b2b468c0e7b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f258c5a-4849-4d95-b682-1b2b468c0e7b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import re\n",
        "import pickle"
      ],
      "metadata": {
        "id": "7LjVPaoFIvAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmBDz-2KIu8R",
        "outputId": "8dda8bc1-d9ed-4741-d671-5a35bbec0522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40001, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document = df['abstract']\n",
        "summary = df['title']"
      ],
      "metadata": {
        "id": "5Wp6u0dUIu5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document[30], summary[30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoguLD97Iu2v",
        "outputId": "35f9ad80-baed-4647-957e-dbb3e6a0d54d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('This paper presents GIS time-series land-use analysis of satellite images to quantify the recovery of rice cultivation and aquaculture following the 2004 Indian Ocean tsunami in coastal communities in Aceh, Indonesia. We supplement this with qualitative data to illustrate the post-disaster challenges faced by residents, and the extent to which coastal communities have adapted to post-tsunami realities. Our analysis shows that the rehabilitation of rice cultivation and aquaculture in areas inundated by the tsunami has been limited by extensive degradation of land, diversion of labor by tsunami mortality and transition to alternative livelihoods, and re-purposing of rice fields for residential use during the reconstruction phase. This is especially prominent in areas where subsistence activities are not the primary source of livelihood. The Aceh case study shows that social, economic, and environmental factors can be stronger determinants of how coastal livelihoods rebound and change following destructive inundation events than livelihood rehabilitation aid. Additionally, our case study suggests the human impact of coastal hazards can be felt outside the physical extent of inundation.',\n",
              " ' Rehabilitating coastal agriculture and aquaculture after inundation events: Spatial analysis of livelihood recovery in post-tsunami Aceh, Indonesia ')"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "I8WdsksAJVol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for decoder sequence\n",
        "summary = summary.apply(lambda x: ' ' + x + ' ')\n",
        "summary.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYpaF2ekIuuw",
        "outputId": "3dabc7ef-ff9c-41d1-f3ae-72753d497f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         PEITC induces apoptosis of Human Brain Gl...\n",
              "1         Multi-step biocatalytic strategies for ch...\n",
              "2         Dengue vaccine safety signal: Immune enha...\n",
              "3         Achieving respectable motherhood? Explori...\n",
              "4         Soft++, a multi-parametric non-saturating...\n",
              "Name: title, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing the texts into integer tokens"
      ],
      "metadata": {
        "id": "7Dv8uv5gJvTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# since < and > from default tokens cannot be removed\n",
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "oov_token = ''"
      ],
      "metadata": {
        "id": "_xqzSTu5Iusv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
        "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)"
      ],
      "metadata": {
        "id": "Avvrthp4Iuql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_tokenizer.fit_on_texts(document)\n",
        "summary_tokenizer.fit_on_texts(summary)"
      ],
      "metadata": {
        "id": "d54Rc3oqJ5WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = document_tokenizer.texts_to_sequences(document)\n",
        "targets = summary_tokenizer.texts_to_sequences(summary)"
      ],
      "metadata": {
        "id": "rXAjtGLNKI3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_tokenizer.texts_to_sequences([\"This is a test\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF3q6zznKRIi",
        "outputId": "dc525e12-8849-44a9-a68e-ff39271d7948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4936, 33, 6, 409]]"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_tokenizer.sequences_to_texts([[4936, 33, 6, 409]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RddvGkSbKW1y",
        "outputId": "87dd89f0-6f81-40dd-ca1b-10b91305f85b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this is a test']"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_vocab_size = len(document_tokenizer.word_index) + 1\n",
        "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
        "\n",
        "# vocab_size\n",
        "encoder_vocab_size, decoder_vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfZRZo2bKguF",
        "outputId": "efb4541d-c8f7-4989-e004-1cd8a766fbaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(149751, 43707)"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtaining insights on lengths for defining maxlen"
      ],
      "metadata": {
        "id": "g8MmCOxAKpn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_lengths = pd.Series([len(x) for x in document])\n",
        "summary_lengths = pd.Series([len(x) for x in summary])"
      ],
      "metadata": {
        "id": "ioaJtZbxKlnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_lengths.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Pw_A_40KzUA",
        "outputId": "2afb468c-b518-4622-cb7c-a9d0a124d39f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    40001.000000\n",
              "mean      1434.145946\n",
              "std        555.491389\n",
              "min          3.000000\n",
              "25%       1069.000000\n",
              "50%       1401.000000\n",
              "75%       1748.000000\n",
              "max       6993.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_lengths.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEoZZLSkKJJh",
        "outputId": "dfa88d96-f41d-4fcf-92e5-7a0196b5a57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    40001.000000\n",
              "mean       104.878078\n",
              "std         35.047157\n",
              "min         11.000000\n",
              "25%         80.000000\n",
              "50%        102.000000\n",
              "75%        126.000000\n",
              "max        378.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# maxlen\n",
        "# taking values > and round figured to 75th percentile\n",
        "# at the same time not leaving high variance\n",
        "#encoder_maxlen = 400\n",
        "#decoder_maxlen = 75\n",
        "\n",
        "encoder_maxlen = 1434\n",
        "decoder_maxlen = 104"
      ],
      "metadata": {
        "id": "AcD6PR7jK5w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding/Truncating sequences for identical sequence lengths"
      ],
      "metadata": {
        "id": "3MktAaXALwdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')\n",
        "\n"
      ],
      "metadata": {
        "id": "jiLJXfvrKJPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating dataset pipeline"
      ],
      "metadata": {
        "id": "xKGs-TnxL8l5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.cast(inputs, dtype=tf.int32)\n",
        "targets = tf.cast(targets, dtype=tf.int32)"
      ],
      "metadata": {
        "id": "JA2ZZzcYL5Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "9u3Ew3E1J5bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "SckjvGJfJ5dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding for adding notion of position among words as unlike RNN this is non-directional\n"
      ],
      "metadata": {
        "id": "es06OrDiMYaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * angle_rates"
      ],
      "metadata": {
        "id": "YIWH4hcNJ5ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "A9I8FwVxMfky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Masking\n",
        "Padding mask for masking \"pad\" sequences\n",
        "Lookahead mask for masking future words from contributing in prediction of current words in self attention"
      ],
      "metadata": {
        "id": "kPdVP_XlMmpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]"
      ],
      "metadata": {
        "id": "AwrWikoTJ5ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask"
      ],
      "metadata": {
        "id": "6OTItd3QMsa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Model\n",
        "Scaled Dot Product"
      ],
      "metadata": {
        "id": "6XWPHMCqM31A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "uiogro4mMsp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Headed Attention"
      ],
      "metadata": {
        "id": "SJEZnpuwNBWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "            \n",
        "        return output, attention_weights"
      ],
      "metadata": {
        "id": "XUzfyBwQM-gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed Forward Network"
      ],
      "metadata": {
        "id": "Rt4pNzzqNLmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ],
      "metadata": {
        "id": "L3-7UZejNH5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fundamental Unit of Transformer encoder"
      ],
      "metadata": {
        "id": "SS-nRv_SNSxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ],
      "metadata": {
        "id": "LbTFw65xMsta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fundamental Unit of Transformer decoder"
      ],
      "metadata": {
        "id": "WiutAYCUNaXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "metadata": {
        "id": "KHB2A63FMswX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder consisting of multiple EncoderLayer(s)"
      ],
      "metadata": {
        "id": "7NNLt0IzNiey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "    \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        return x"
      ],
      "metadata": {
        "id": "6QItttkVMsy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder consisting of multiple DecoderLayer(s)"
      ],
      "metadata": {
        "id": "lLrIIN2mNqeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "        return x, attention_weights"
      ],
      "metadata": {
        "id": "yzGQNl5FMs4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finally, the Transformer"
      ],
      "metadata": {
        "id": "oEXxvZMUN0Zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "metadata": {
        "id": "pK1mFMOvNxHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ZKmMFdzNN9Jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyper-params\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "EPOCHS = 5"
      ],
      "metadata": {
        "id": "XgIO3rgZN6WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adam optimizer with custom learning rate scheduling"
      ],
      "metadata": {
        "id": "2eafsvCEOGHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "PaURp8uZMs9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining losses and other metrics"
      ],
      "metadata": {
        "id": "YZ1VkAXcON_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "metadata": {
        "id": "v21BfPTIMtBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ],
      "metadata": {
        "id": "_ZF5fKlXJ5sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "N1u_8SQbJ5ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ],
      "metadata": {
        "id": "ZDZcevAmJ5yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "NmGVdULaOgEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(\n",
        "    num_layers, \n",
        "    d_model, \n",
        "    num_heads, \n",
        "    dff,\n",
        "    encoder_vocab_size, \n",
        "    decoder_vocab_size, \n",
        "    pe_input=encoder_vocab_size, \n",
        "    pe_target=decoder_vocab_size,\n",
        ")"
      ],
      "metadata": {
        "id": "tGFF2Y07Omi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masks"
      ],
      "metadata": {
        "id": "8CvEX1J9Oq8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "metadata": {
        "id": "O_4WNaKZJ50L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checkpoints"
      ],
      "metadata": {
        "id": "VhNr8DY1Ox2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"checkpoints\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "metadata": {
        "id": "NuQtzS6OJ54C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6c53c99-102c-4d82-b349-0ff7a79764e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest checkpoint restored!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training steps"
      ],
      "metadata": {
        "id": "TlZh9wg0PNYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, \n",
        "            True, \n",
        "            enc_padding_mask, \n",
        "            combined_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ],
      "metadata": {
        "id": "kePWSb9FO4Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "    \n",
        "        # 55k samples\n",
        "        # we display 3 batch results -- 0th, middle and last one (approx)\n",
        "        # 55k / 64 ~ 858; 858 / 2 = 429\n",
        "        if batch % 429 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
        "      \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1bjDJBeJ59f",
        "outputId": "fcc8732a-1318-4401-9ec9-45760a13d660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.4302\n",
            "Epoch 1 Batch 429 Loss 2.6339\n",
            "Epoch 1 Loss 2.7434\n",
            "Time taken for 1 epoch: 315.0805377960205 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.2651\n",
            "Epoch 2 Batch 429 Loss 2.5821\n",
            "Epoch 2 Loss 2.6934\n",
            "Time taken for 1 epoch: 300.2356572151184 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.3728\n",
            "Epoch 3 Batch 429 Loss 2.5319\n",
            "Epoch 3 Loss 2.6396\n",
            "Time taken for 1 epoch: 300.21465253829956 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 2.2955\n",
            "Epoch 4 Batch 429 Loss 2.4854\n",
            "Epoch 4 Loss 2.5949\n",
            "Time taken for 1 epoch: 300.2170763015747 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 2.0718\n",
            "Epoch 5 Batch 429 Loss 2.4359\n",
            "Saving checkpoint for epoch 5 at checkpoints/ckpt-6\n",
            "Epoch 5 Loss 2.5455\n",
            "Time taken for 1 epoch: 300.9813542366028 secs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "## Predicting one word at a time at the decoder and appending it to the output; then taking the complete sequence as an input to the decoder and repeating until maxlen or stop keyword appears"
      ],
      "metadata": {
        "id": "Tu3R_TCnbQHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(input_document):\n",
        "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "\n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index[\"\"]]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(decoder_maxlen):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input, \n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == summary_tokenizer.word_index[\"\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n"
      ],
      "metadata": {
        "id": "LzrMRu0hJ6Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(input_document):\n",
        "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # not printing  token\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ],
      "metadata": {
        "id": "S95Mm5UUJ6Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize(\n",
        "    \"This paper presents GIS time-series land-use analysis of satellite images to quantify \\\n",
        "     the recovery of rice cultivation and aquaculture following the 2004 Indian Ocean tsunami \\\n",
        "      in coastal communities in Aceh, Indonesia. We supplement this with qualitative data to \\\n",
        "       illustrate the post-disaster challenges faced by residents, and the extent to which coastal \\\n",
        "        communities have adapted to post-tsunami realities. Our analysis shows that the rehabilitation \\\n",
        "         of rice cultivation and aquaculture in areas inundated by the tsunami has been limited by \\\n",
        "          extensive degradation of land, diversion of labor by tsunami mortality and transition to \\\n",
        "           alternative livelihoods, and re-purposing of rice fields for residential use during the \\\n",
        "            reconstruction phase. This is especially prominent in areas where subsistence activities \\\n",
        "             are not the primary source of livelihood. The Aceh case study shows that social, economic, and \\\n",
        "              environmental factors can be stronger determinants of how coastal livelihoods rebound and \\\n",
        "               change following destructive inundation events than livelihood rehabilitation aid. \\\n",
        "                Additionally, our case study suggests the human impact of coastal hazards can be felt outside \\\n",
        "                 the physical extent of inundation.\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "4JdTRi0iJ6IH",
        "outputId": "0b47551f-ebdf-4b02-ff16-e4453496ef96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'of east asia water resources for the united kingdom population based on maternal mortality and flood protection in germany spain the united states usa from tanzania the united states transfer initiative of india 2016 national surveys from the case study of nepal river basin of india 2016 case study of nepal storm and case studies of nepal earthquake protection program in england and south africa data from south africa wide island coast based on local case studies of nepal storm region level data from the european states management case definition of nepal storm area level data from south africa data from the european union'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = []\n",
        "for d in document[:20]:\n",
        "  summaries.append([summarize(d)])"
      ],
      "metadata": {
        "id": "0slDAi-7ZTJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFltML1Ga1vQ",
        "outputId": "8df94f52-2f48-459d-c94d-d61d13e2aa73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['promotes the presence of extracellular vesicles the liver injury and cancer cell cycle of their breast cancer in mice and the elderly regulation of the mtor pathway based cancer cell lines in ageing mice a family planning a family planning network meta analysis of law and cancer patient with oecd cancer patient with oecd cancer patient with oecd modified cells cross sectional features of web based therapies meta analyses of law cancer patient with rise regulation strategy in rise mechanisms structural liver cancer patient with law cancer patient with complete deficiency german society new zealand doped cells independent cancer stem cell cycle arrest view'],\n",
              " ['of green eu care products based on the green synthesis of synthetic biology solutions of pseudomonas aeruginosa evolutionary tools used by use of resources structural analysis by d based ligand stable isotopes in virtual screening based ligand structure and machine learning methods and machine learning strategy for drug source group based conservation units based conservation based conservation based conservation based kinetic calculation software security index ligand adsorption operations biology approaches optimized methods optimized methods optimized methods optimized methods synthesis data on artificial intelligence class ii engineering tools used as viable ionic liquid based ligand structure engineering tools used as an example from new flood'],\n",
              " ['of the live attenuated vaccine formulation in an australian health system a retrospective analysis of free vaccines randomized controlled trials of new zealand english species co infected adults and clinical trials for hospitals in vietnam vaccine effectiveness of new vaccination coverage trials design considerations reviews was developed countries vaccine safety assessments based vaccines against group based vaccines against the netherlands vaccination coverage acting vaccine safety assessments of new european countries vaccine effectiveness for new vaccination coverage defining the netherlands fever vaccine safety issues derived the netherlands vaccination group a global south africa live vaccines against the global warming vaccine trials group a holistic and'],\n",
              " ['of the green vegetables and the need for use of modified ovarian cancer infections in pregnant women and their infants implications for research from the world health facility cluster de work in mass spectrometry de products peru in mass spectrometry nepal earthquake safety concerns change research centre working group  eye nematode archives structure theory study protocol implementation of italy seismic cloud service industry centre standard era an english frontier canada poland  asian private cluster de support group  asian countries asian university industry canada poland  asian canada poland  asian countries  an english new zealand english new restriction agenda setting'],\n",
              " ['of the overall system for an automated model of unknown entity over an open source software center randomized controlled trial with it individual response statistics response function analysis and non clinical trial designs in clinical trials in clinical trials statistics network analyses of group trials in clinical trials in an endangered group of group trials group decision making group of the network parameters that clinical trials group data driven algorithm when non clinical trials for clinical trials in clinical trials group i trial when using the european society for variable access to study decision making group case control trials framework for clinical trials use'],\n",
              " [\"of the low density lipoprotein cholesterol from the green form of animal feeds in adults with cystic fibrosis  more effective water magnetic resonance imaging shows an example from the asia g vii lineage analysis of the center and asia g vii reconstruction schemes of finland birth case series of gulf of italy and asia g vii women's survey experiment and silver modified search strategy in finland birth cohort data from two case series stacks for research centre in canada globally an invasive procedures women's d nepal himalaya region in canada mexico patient with three case definition of the german commission on the german\"],\n",
              " ['of microbial fuel cells for use of improved charge storage in preparation performance assessments modified with hybrid turning genomic materials and flight temperature data from birth cohorts for hospitals of mice with porous media used in preparation on trial design board monitoring applications of vietnam testing facility based on clinical trials performance characterization of commercial loaded preparation and farmers of modified diamond preparation for clinical data from commercial medicine ag tio2 nanocomposite preparation for clinical trials performance characterization of commercial cigarettes based semi open label free scan proton beam irradiation dose preparation reference biomedical applications in three dimensional melt grown on novel polymer electrolyte'],\n",
              " ['of the green algae consortium in a large scale dairy region a review of the literature and a new point process view of their diagnostic tests for european health care products in pregnancy management studies on european cities of pregnant women living safety climate reconstruction schemes in pregnancy management studies of south african and non cystic fibrosis statistics and non psychiatric disorders caused by oral cancer patients with cystic fibrosis statistics high value chains adults with young adults with cystic fibrosis in the european society the european society the european case definition of three case definition guidelines in 30 years of three case definition'],\n",
              " [\"of primary care bone marrow patients with suspected cancer risk in the era of clinical populations a prospective observational study of the european prospective observational prospective observational diagnostic diagnostic diagnostic diagnostic project in england and clinical trials for clinical trials for england and clinical trials for pregnancy case series of experts in infancy for research agenda setting guidelines for clinical trials for clinical trials for clinical trials la trials la trials la trials for clinical trials in pregnancy patients and adult patients in anaesthesia and non clinical trials use in anaesthesia and adult onset alzheimer's disease patient classification of new zealand patients with clinically\"],\n",
              " ['of the breast cancer cell lines is affected by the same reaction initiative with its consequences for the management of fracture risk factors a meta analysis of behavioural interventions in pregnant women living safety studies investigating the smart safety climate services from two randomized controlled trials in pregnant women living safety studies with wild type 2 patient safety pharmacology to treat randomized controlled trial safety pharmacology practice teacher education program risk management studies investigating the european society of male patient safety practice in practice theory of practice for male female patients with generalized intensive care setting cell health advice 13 month old male depression'],\n",
              " ['of the green city scale made from an energy system perspective on the internet of things tumor data and the uk green city of german and non animal surveys in the uk dairy surveys mining industry canada mexico city centre industry policy industry  community based on publicly available policy implications for research centre industry leading to support fly ash industry leading to industrial data collection industrial data from new zealand location problem an industry case study decision making industry case study decision making industry policy industry canada mexico city centre industry case study decision making initiative management projects decision making industry case study'],\n",
              " ['of the sea wall waters an experimental study of the european union with cystic fibrosis performance of five different enrichment in two day old children living animal welfare systems toxicology provinces of the common bean phaseolus vulgaris group in japan in japan implications for three case definition of south africa river delta search scheme on northeast china sea cucumber plants used for three routes in japan italy wear pathogenic elements the gulf cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation cooperation focus on peanut'],\n",
              " ['promotes the progression of primary microglial cell carcinoma via selective autophagy induced structural defects in mice and mice after severe weather like environments with stable isotopes of mice in mice with mice in mice and mice in vitro motor impairment in vivo policies deletion syndrome mice with mice with dependent manner distinct neurodevelopmental disorders with dependent manner distinct neurodevelopmental disorders caused by flood affected antiviral features of mice with mice with flood affected zone birth with mice with flood like kinases activation with flood dependent manner distinct independent features of flood affected zone planning that g2019s lrrk2 condition disruption in mice in rise related'],\n",
              " ['of the japanese phenotype of common dna damage and non animal models of allergic diseases among women with animal disease caused by steroid hormones in the european society of parents and non alcoholic steatohepatitis linked ages 12 months gene polymorphisms region liver disease patient data from the european society of pregnant women with atopic eczema animal health system liver disease patient registry data cube polymorphisms region polymorphisms group cross sectional variants eastern canada globally machine learning tower a new zealand coefficient gene polymorphisms research centre network a case definition of pregnant women living law liver disease outbreak variants eastern canada molecular variants eastern canada'],\n",
              " ['of health care products and the ratio of polyethylene glycol mixture in an urban area study using oral cholera vaccine water in china a randomized controlled trial in western kenya study of india pilot study protocol in china and pre heating and pre heating survey experiment results from the united states health worker health facility cluster randomised controlled trial of united kingdom study protocol on canada mexico city study on canada mexico city the united states water safety data from the united states water safety issues water safety issues water safety results from the united states water safety features of the united states water'],\n",
              " ['of care power an example from the uk primary care unit a retrospective cohort study of electronic health records data from the uk biobank registry and a case control registry data from the uk database analysis protocol to inform led network analysis protocol and case study of poland and case study of 2018 retrospective database analysis protocol to inform surveillance network consultation for 2016 database driven screening classification programme to 2016 patient data from two case series of electronic health record based on access and update protocol in practice based pregnancy registry data from two case control programme for routine cases of collaboration on'],\n",
              " ['of the global south african liver a review of the literature and genetic assessment of use in mass spectrometry nepal evolutionary approaches and data from the global south african health system of italy and matrix cross sectional surveys in mass spectrometry group of new panel data from new zealand kenya in mass health program science surveillance system based vaccines based vaccines based vaccines based vaccines based on mass spectrometry group a new panel data from two new zealand group of research agenda setting data from two italian south asia g vii streptococcus pneumoniae data from two case definition of new pregnancy surveillance techniques in'],\n",
              " ['of the japanese oak plateau snow lines that is effective for biomedical applications in the indian ocean sector of india canada current evidence from the german basin of india 2016 national surveys from the european commission joint on remote sensing grid and case study 2016 national safety data from the european commission programme to 2016 region of 10 1016 j region of 2013 2014 2013 2014 2013 2014 coast fever vaccine wear surveillance data from the european commission bearing steels for characterising section shanghai china sea region in collaboration leading to 2016 projects registry data from new smoking cessation crisis on publicly available onset'],\n",
              " ['of the bridge composite materials that influence the bridge of used in the uk a comprehensive review of existing methods for research on bridge safety and resource utilisation in virtual research agenda and male female research agenda of testing data centres material safety analyses of testing data centres of testing groups types for research agenda of non animal welfare functions in three case studies applications in three case studies on bridge engineering studies of testing practice practice practice practice practice practice practice practice practice theories for bridge engineering issues near al issues near al issues near al management tools used in three cases of'],\n",
              " ['of the small area economy of the green algae industry caused by flood exposures to an immunocompetent male circumcision case control policy perspective on the english community based medicine in china a community based policy agenda for resource poor health sector in three case control programme decline of new zealand english national health sector in china sea mining region the northern adriatic sea mining region the northern finland birth cohort new mexico city addressing new mexico city the northern finland case definition of the northern finland practice german basin  an english national jurisdiction onset pathogen case definition guidelines from the blue mountains eye']]"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "from nltk.translate.bleu_score import sentence_bleu\n"
      ],
      "metadata": {
        "id": "hhfS3uN1b-OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of BLEU"
      ],
      "metadata": {
        "id": "F-8ezUC7hDZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = summary[0]\n",
        "candidate = summaries[0]\n",
        "candidate_1 = ' '.join(candidate)\n",
        "score = sentence_bleu(reference, candidate_1)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZUlH7c_dDVH",
        "outputId": "d36ec787-a8b1-48a4-f352-c39d12d2f74a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.778565201456134e-232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "for i in range(20):\n",
        "    reference = summary[i]\n",
        "    candidate = summaries[i]\n",
        "    candidate_1 = \" \".join(candidate)\n",
        "    score = sentence_bleu(reference, candidate_1)\n",
        "    scores.append(score)\n",
        "print(\"Average Blue scores for 20 summaries\", np.mean(scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaLPpWFY5K6L",
        "outputId": "1bd9578b-551e-4390-c250-c2c4072a23d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Blue scores for 20 summaries 7.48492284824238e-232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of ROUGE"
      ],
      "metadata": {
        "id": "LgVVHbU-hJhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r rouge/requirements.txt\n",
        "!pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXD7QCr-hI4N",
        "outputId": "4fa6d6ec-aa41-4b51-fa7c-b73777ae5b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'rouge/requirements.txt'\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.7)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.3.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.21.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (2022.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(candidate_1, reference)\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcENTZ70ieUk",
        "outputId": "dda06515-b2b5-42cb-a215-c42ec076e5dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': Score(precision=0.3888888888888889, recall=0.06796116504854369, fmeasure=0.11570247933884296), 'rougeL': Score(precision=0.2777777777777778, recall=0.04854368932038835, fmeasure=0.08264462809917356)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = {}\n",
        "for i in range(20):\n",
        "    reference = summary[i]\n",
        "    candidate = summaries[i]\n",
        "    candidate_1 = \" \".join(candidate)\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    scores_rouge = scorer.score(candidate_1, reference)\n",
        "    #score_fuzz = fuzz.partial_ratio(reference,candidate_1)\n",
        "    #score = sentence_bleu(reference, candidate_1)\n",
        "    scores['rouge1'].append(scores_rouge)\n",
        "    #scores.append(scores_rouge)\n",
        "print(\"Average scores_rouge for 20 summaries\", scores['rouge1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "jOasH6NT8Giz",
        "outputId": "02ea0aed-3739-446b-c978-ce557f52b9c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-230-b55c0f6e46dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#score_fuzz = fuzz.partial_ratio(reference,candidate_1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#score = sentence_bleu(reference, candidate_1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rouge1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_rouge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m#scores.append(scores_rouge)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average scores_rouge for 20 summaries\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rouge1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'rouge1'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation of Match Ratio Score\n"
      ],
      "metadata": {
        "id": "w6b9lGe2hP5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD5MaX46lxlw",
        "outputId": "c2d94cf9-5c78-41c6-c7f8-b957c3882e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process"
      ],
      "metadata": {
        "id": "g1mPXXvhJ6W8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d02f636-e20d-4f16-eb63-2cd042dabf68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(fuzz.ratio(reference,candidate_1))\n",
        "#91\n",
        "print(fuzz.partial_ratio(reference,candidate_1)) ## best\n",
        "#100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA87S1B3lvY3",
        "outputId": "38fa719d-c18a-4ae1-d2c8-75d1907502d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "for i in range(20):\n",
        "    reference = summary[i]\n",
        "    candidate = summaries[i]\n",
        "    candidate_1 = \" \".join(candidate)\n",
        "    score_fuzz = fuzz.partial_ratio(reference,candidate_1)\n",
        "    #score = sentence_bleu(reference, candidate_1)\n",
        "    scores.append(score_fuzz)\n",
        "print(\"Average Fuzz scores for 20 summaries\", np.mean(scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5eroNXi7ggE",
        "outputId": "ee426845-613f-4c66-9df1-a7fca5719258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Fuzz scores for 20 summaries 33.25\n"
          ]
        }
      ]
    }
  ]
}